{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99f8090c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyaudio\n",
    "import time\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa02a4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-8da2ccac033e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[0maudio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mAudioHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[0maudio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m     \u001b[1;31m# open the the stream\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m \u001b[0maudio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmainloop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# main operations with librosa\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m \u001b[0maudio\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-8da2ccac033e>\u001b[0m in \u001b[0;36mmainloop\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ok'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m             \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class AudioHandler(object):\n",
    "    def __init__(self):\n",
    "        self.FORMAT = pyaudio.paFloat32\n",
    "        self.CHANNELS = 1\n",
    "        self.RATE = 44100\n",
    "        self.CHUNK = 1024 * 2\n",
    "        self.RECORD_SECONDS = 1\n",
    "        self.INPUT_DEVICE_INDEX = 1 # change this parametr to good work\n",
    "        self.p = None\n",
    "        self.stream = None\n",
    "\n",
    "    def start(self):\n",
    "        self.p = pyaudio.PyAudio()\n",
    "        self.stream = self.p.open(format=self.FORMAT,\n",
    "                                  channels=self.CHANNELS,\n",
    "                                  rate=self.RATE,\n",
    "                                  input=True,\n",
    "                                  output=False,\n",
    "                                  #stream_callback=self.callback,\n",
    "                                  frames_per_buffer=self.CHUNK,\n",
    "                                  input_device_index = self.INPUT_DEVICE_INDEX)\n",
    "\n",
    "    def stop(self):\n",
    "        self.stream.close()\n",
    "        self.p.terminate()\n",
    "\n",
    "    def callback(self, in_data, frame_count, time_info, flag):\n",
    "        numpy_array = np.frombuffer(in_data, dtype=np.float32)\n",
    "        librosa.feature.mfcc(numpy_array)\n",
    "        return None, pyaudio.paContinue\n",
    "\n",
    "    def mainloop(self):\n",
    "        while (self.stream.is_active()): # if using button you can set self.stream to 0 (self.stream = 0), otherwise you can use a stop condition\n",
    "            model = load_model(r'D:\\save_1.h5')\n",
    "            \n",
    "            frames = []  \n",
    "            for i in range(0, int(self.RATE / self.CHUNK * self.RECORD_SECONDS)):\n",
    "                data = self.stream.read(self.CHUNK)\n",
    "                frames.append(data)   \n",
    "            frame = b''.join(frames)\n",
    "            callback(self, frame, len(frames), )\n",
    "            print('ok')\n",
    "                \n",
    "            time.sleep(2.0)\n",
    "\n",
    "\n",
    "audio = AudioHandler()\n",
    "audio.start()     # open the the stream\n",
    "audio.mainloop()  # main operations with librosa\n",
    "audio.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c35223d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_spectrogram(y, sample_rate):\n",
    "    \n",
    "    plt.interactive(False)\n",
    "    #clip, sample_rate = librosa.load(filename, sr=None)\n",
    "        \n",
    "    fig = plt.figure(figsize=[0.72,0.72])\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.axes.get_xaxis().set_visible(False)\n",
    "    ax.axes.get_yaxis().set_visible(False)\n",
    "    ax.set_frame_on(False)\n",
    "    spectrogram = librosa.feature.melspectrogram(y=y, sr=sample_rate)\n",
    "    librosa.display.specshow(librosa.power_to_db(spectrogram, ref=np.max))\n",
    "\n",
    "    \n",
    "    \n",
    "    #filename = path\n",
    "    #plt.savefig(filename, dpi=400, bbox_inches='tight',pad_inches=0)\n",
    "    \n",
    "    \n",
    "    plt.close()    \n",
    "    fig.clf()\n",
    "    plt.close(fig)\n",
    "    plt.close('all')\n",
    "    \n",
    "    return spectrogram\n",
    "    del filename,clip,sample_rate,fig,ax,S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5bed841a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "\n",
    "def feature_extract(y, sr):\n",
    "    \"\"\"\n",
    "    Define function that takes in a file an returns features in an array\n",
    "    \"\"\"\n",
    "    \n",
    "    #get wave representation\n",
    "    #y, sr = librosa.load(file)\n",
    "    #print(y)    \n",
    "    #determine if instruemnt is harmonic or percussive by comparing means\n",
    "    y_harmonic, y_percussive = librosa.effects.hpss(y)\n",
    "    if np.mean(y_harmonic)>np.mean(y_percussive):\n",
    "        harmonic=1\n",
    "    else:\n",
    "        harmonic=0\n",
    "        \n",
    "    #Mel-frequency cepstral coefficients (MFCCs)\n",
    "    mfcc = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "    #temporal averaging\n",
    "    mfcc=np.mean(mfcc,axis=1)\n",
    "    \n",
    "    #get the mel-scaled spectrogram\n",
    "    spectrogram = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128,fmax=8000)  \n",
    "    #temporally average spectrogram\n",
    "    spectrogram = np.mean(spectrogram, axis = 1)\n",
    "    \n",
    "    #compute chroma energy\n",
    "    chroma = librosa.feature.chroma_cens(y=y, sr=sr)\n",
    "    #temporally average chroma\n",
    "    chroma = np.mean(chroma, axis = 1)\n",
    "    \n",
    "    #compute spectral contrast\n",
    "    contrast = librosa.feature.spectral_contrast(y=y, sr=sr)\n",
    "    contrast = np.mean(contrast, axis= 1)\n",
    "    \n",
    "    return [harmonic, mfcc, spectrogram, chroma, contrast]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "496f4115",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* recording\n",
      "* done recording\n",
      "<class 'bytes'>\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paFloat32\n",
    "CHANNELS = 2\n",
    "RATE = 44100\n",
    "RECORD_SECONDS = 1\n",
    "WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "\n",
    "p = pyaudio.PyAudio()\n",
    "\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                frames_per_buffer=CHUNK)\n",
    "\n",
    "print(\"* recording\")\n",
    "\n",
    "frames = []\n",
    "\n",
    "for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data)\n",
    "\n",
    "print(\"* done recording\")\n",
    "# print(type(frames))\n",
    "# print(type(frames[0]))\n",
    "# print(frames[0])\n",
    "\n",
    "\n",
    "all_frames = b''.join(frames)\n",
    "print(type(all_frames))\n",
    "\n",
    "# byte_sample = []\n",
    "# for chunk in frames: \n",
    "#     byte_sample.append(chunk)\n",
    "\n",
    "# print(type(byte_sample[0]))\n",
    "# print(byte_sample[0])\n",
    "\n",
    "numpy_array = np.frombuffer(all_frames, dtype=np.float32)\n",
    "#print(librosa.feature.mfcc(numpy_array))\n",
    "#print(numpy_array)\n",
    "feature = feature_extract(numpy_array, RATE)\n",
    "#print(feature)\n",
    "sp = create_spectrogram(numpy_array, RATE)\n",
    "\n",
    "\n",
    "# import matplotlib.pyplot as plt\n",
    "# fig, ax = plt.subplots()\n",
    "# img = librosa.display.specshow(feature[0], x_axis='time', ax=ax)\n",
    "# fig.colorbar(img, ax=ax)\n",
    "# ax.set(title='MFCC')\n",
    "\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    "\n",
    "\n",
    "wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "wf.setnchannels(CHANNELS)\n",
    "wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "wf.setframerate(RATE)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e153ae5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3.97069311e-09 2.41422149e-09 3.83781753e-06 ... 1.23305679e+02\n",
      "  5.33592186e+01 1.51112785e+01]\n",
      " [1.38279799e-09 3.14439719e-09 6.37267203e-06 ... 2.40405457e+02\n",
      "  1.09371216e+02 4.67931137e+01]\n",
      " [8.40854997e-10 6.09344686e-10 8.29826149e-06 ... 6.96133270e+01\n",
      "  1.72022438e+01 9.78102016e+00]\n",
      " ...\n",
      " [5.88393112e-09 5.01833108e-09 3.38652590e-08 ... 1.48421077e-05\n",
      "  1.67465914e-05 2.08872098e-05]\n",
      " [5.80504711e-09 7.03017955e-09 3.55521550e-08 ... 1.60732889e-04\n",
      "  7.36468064e-05 6.42417799e-05]\n",
      " [8.71521433e-09 9.29739485e-09 1.81104589e-08 ... 5.98095474e-04\n",
      "  2.01462477e-04 1.32240239e-04]]\n"
     ]
    }
   ],
   "source": [
    "print(sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2c8949f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.QuadMesh at 0x250d6eb05e0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "librosa.display.specshow(librosa.power_to_db(sp, ref=np.max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc683c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00170791 0.9983909 ]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from data_prep import valik\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image, ImageFont, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "path = load_model(r'D:\\save_1.h5')\n",
    "\n",
    "model = load_model('/home/aigaf/Downloads/Telegram Desktop/save_1.h5')\n",
    "\n",
    "\n",
    "# path_to_img = 'cut1.jpg'\n",
    "# img = Image.open(path_to_img)\n",
    "# img_arr = np.asarray(img)\n",
    "img_arr = sp\n",
    "# print(img_arr.shape, img_arr)\n",
    "img_norm = img_arr/255\n",
    "img_size = img_norm.resize((150, 150, 3))\n",
    "img_compl = img_norm.reshape(-1, 150, 150, 3)\n",
    "# print(img_compl)\n",
    "\n",
    "#results = model.evaluate(valik)\n",
    "#print('loss, accuracy =', results)\n",
    "\n",
    "\n",
    "final_res = model.predict(img_compl)\n",
    "print(final_res)\n",
    "# data = {'y_Actual':    [ tuple(i) for i in valik[1][1]],\n",
    "#         'y_Predicted': [ tuple(i) for i in final_res]\n",
    "#         }\n",
    "#\n",
    "# df = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
    "# print (df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4126b6bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "0d537df178d4a96c2cf8ca99df555b0d4bf3b31be3996b0ee2dcd6eb18f548f4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
