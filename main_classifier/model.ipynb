{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# data preparation\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from tensorflow.keras import backend\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from keras import models\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import keras.backend as K\n",
    "import librosa\n",
    "import librosa.display\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import figure\n",
    "import gc\n",
    "from path import Path\n",
    "from keras_preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0/255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        r'D:\\reposetory\\Save_Transport\\dataset\\train',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20)\n",
    "\n",
    "valik = test_datagen.flow_from_directory(\n",
    "        r'D:\\reposetory\\Save_Transport\\dataset\\test',\n",
    "        target_size=(150, 150),\n",
    "        batch_size=1\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "24/24 - 9s - loss: 0.4603 - accuracy: 0.8214 - val_loss: 0.4809 - val_accuracy: 0.6667\n",
      "Epoch 2/10\n",
      "24/24 - 9s - loss: 0.2794 - accuracy: 0.8382 - val_loss: 0.3601 - val_accuracy: 0.6667\n",
      "Epoch 3/10\n",
      "24/24 - 9s - loss: 0.2976 - accuracy: 0.8125 - val_loss: 0.1714 - val_accuracy: 1.0000\n",
      "Epoch 4/10\n",
      "24/24 - 10s - loss: 0.2768 - accuracy: 0.8167 - val_loss: 0.2381 - val_accuracy: 1.0000\n",
      "Epoch 5/10\n",
      "24/24 - 9s - loss: 0.2466 - accuracy: 0.8571 - val_loss: 0.0988 - val_accuracy: 1.0000\n",
      "Epoch 6/10\n",
      "24/24 - 9s - loss: 0.2611 - accuracy: 0.8354 - val_loss: 0.4707 - val_accuracy: 0.6667\n",
      "Epoch 7/10\n",
      "24/24 - 8s - loss: 0.2729 - accuracy: 0.8298 - val_loss: 0.2412 - val_accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# model\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "def weighted_binary_crossentropy(y_true, y_pred):\n",
    "    weights = (tf.math.abs(y_true-1) * 59.) + 1.\n",
    "    bce = tf.keras.losses.binary_crossentropy(y_true, y_pred)\n",
    "    weighted_bce = K.mean(bce * weights)\n",
    "    return weighted_bce\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, (3, 3), input_shape=(150, 150, 3)),\n",
    "    tf.keras.layers.Conv2D(16, (3, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.3),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3)),\n",
    "    tf.keras.layers.Conv2D(32, (3, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.3),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3)),\n",
    "    tf.keras.layers.Conv2D(64, (3, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2, 2),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.3),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512),\n",
    "    tf.keras.layers.LeakyReLU(alpha=0.3),\n",
    "    tf.keras.layers.Dense(2, activation='sigmoid')\n",
    "])\n",
    "\n",
    "\n",
    "# Настраиваем гиперпараметры нейронной сети\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Начинаем обучение\n",
    "history = model.fit(train_generator,\n",
    "                      validation_data=valik,\n",
    "                      steps_per_epoch=24,\n",
    "                      epochs=10,\n",
    "                      validation_steps=3,\n",
    "                      verbose=2)\n",
    "\n",
    "model.save('save_1.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#from data_prep import valik\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from PIL import Image, ImageFont, ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "SavedModel file does not exist at: D:\\reposetory\\Save_Transport\\main_classifier\\save_1.h5/{saved_model.pbtxt|saved_model.pb}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9c4d02534b4d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'D:\\reposetory\\Save_Transport\\main_classifier\\save_1.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalik\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\n loss, accuracy ='\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    209\u001b[0m       \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    112\u001b[0m                   (export_dir,\n\u001b[1;32m    113\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: D:\\reposetory\\Save_Transport\\main_classifier\\save_1.h5/{saved_model.pbtxt|saved_model.pb}"
     ]
    }
   ],
   "source": [
    "model = load_model(r'D:\\reposetory\\Save_Transport\\main_classifier\\save_1.h5')\n",
    "\n",
    "\n",
    "results = model.evaluate(valik)\n",
    "print('\\n loss, accuracy =', results, '\\n')\n",
    "\n",
    "\n",
    "filenames = valik.filenames\n",
    "count_samples = len(filenames)\n",
    "predict = model.predict_generator(valik,steps = count_samples)\n",
    "\n",
    "\n",
    "\n",
    "#final_res = model.predict(valik[1][0])\n",
    "#print(final_res)\n",
    "\n",
    "#data = {'y_Actual':    [ tuple(i) for i in valik[1][1]],\n",
    "#        'y_Predicted': [ tuple(i) for i in final_res]\n",
    "#        }\n",
    "\n",
    "#df = pd.DataFrame(data, columns=['y_Actual','y_Predicted'])\n",
    "#print (df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "random.seed(228)\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('/home/aigaf/Downloads/Telegram Desktop/save_1.h5')\n",
    "#model = load_model(r'D:\\save_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 60 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "n = 30\n",
    "generator = ImageDataGenerator(rescale=1./255)\n",
    "predict_generator = generator.flow_from_directory(r'D:\\reposetory\\Save_Transport\\dataset\\test',\n",
    "                                               target_size=(150, 150),  shuffle = True, batch_size = n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPredictOrLabels(predictOrLabels, generator):\n",
    "    '''true == labels, false == predict'''\n",
    "    \n",
    "    amountOfBatches = predict_generator.samples / n\n",
    "    amountOfBatches = round(amountOfBatches)\n",
    "    array = []\n",
    "    \n",
    "    for i in range(amountOfBatches):\n",
    "        if predictOrLabels == False:\n",
    "            tmp = model.predict(predict_generator[i][0])\n",
    "        else:\n",
    "            tmp = predict_generator[i][1]\n",
    "\n",
    "        array.extend(tmp)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDataForMetrics(generator):\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    \n",
    "    y_true = getPredictOrLabels(True, generator)\n",
    "    y_pred = getPredictOrLabels(False, generator)\n",
    "    \n",
    "    y_pred =np.argmax(y_pred, axis=1)\n",
    "    y_true = np.argmax(y_true, axis=1)\n",
    "    \n",
    "    return y_true, y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true,y_pred = getDataForMetrics(predict_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9833333333333333"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_true,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[29,  1],\n",
       "       [ 0, 30]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
